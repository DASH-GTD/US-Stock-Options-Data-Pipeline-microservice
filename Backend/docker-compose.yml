x-airflow-common:
  &airflow-common
    build:
      context: .
      dockerfile: Dockerfile-airflow
    environment:
      &airflow-common-env
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__CORE__SQL_ALCHEMY_POOL_ENABLED: 'True'
      AIRFLOW__CORE__SQL_ALCHEMY_POOL_SIZE: 5
      AIRFLOW__CORE__SQL_ALCHEMY_MAX_OVERFLOW: 10
      AIRFLOW__CORE__SQL_ALCHEMY_POOL_RECYCLE: 1800
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./ca.pem:/ca.pem
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
      kafka:
        condition: service_started
services:
  airflow:
    <<: *airflow-common
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    entrypoint: ["/entrypoint.sh"]
    networks:
      - stock_pipeline

  load_balancer:
    image: nginx:latest
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "80:80"
    depends_on:
      - data_collector
    networks:
      - stock_pipeline

  data_collector:
    build:
      context: ./data_collector_service
      dockerfile: Dockerfile
    environment:
      - TWELVE_DATA_API_KEY=200f5a7b095d40ab84753210bc4a95fa
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPICS_DAILY=daily
      - KAFKA_TOPICS_15MIN=15min
      - KAFKA_TOPICS_OPTIONS=options
      - DJANGO_SECRET_KEY="django-insecure-k^c404!2*woj(h(+ek*e#=0sh^qrjw9y-g34m^*qy=^=!43v%^"
    ports:
      - "8000:8000"
    depends_on:
      - kafka
    networks:
      - stock_pipeline
  
  data_processor:
    build:
      context: ./data_processor_service
      dockerfile: Dockerfile
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPICS_DAILY=daily
      - KAFKA_TOPICS_15MIN=15min
      - KAFKA_TOPICS_OPTIONS=options
      - KAFKA_TOPICS_PROCESSED_DAILY=processed-daily
      - KAFKA_TOPICS_PROCESSED_15MIN=processed-15min
      - KAFKA_TOPICS_PROCESSED_OPTIONS=processed-options
    ports:
      - "8001:8001"
    depends_on:
      - kafka
    networks:
      - stock_pipeline

  database_writer:
    build:
      context: ./database_writer_service
      dockerfile: Dockerfile
    environment:
      - AZURE_SQL_CONNECTION_STRING=${AZURE_SQL_CONNECTION_STRING}
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPICS_DAILY=daily
      - KAFKA_TOPICS_15MIN=15min
      - KAFKA_TOPICS_OPTIONS=options
      - KAFKA_TOPICS_PROCESSED_DAILY=processed-daily
      - KAFKA_TOPICS_PROCESSED_15MIN=processed-15min
      - KAFKA_TOPICS_PROCESSED_OPTIONS=processed-options
      - INFLUXDB_URL=https://us-east-1-1.aws.cloud2.influxdata.com
      - INFLUXDB_TOKEN=p4vPDiJyynco8tjaNhG2ch7A51SHtzN0ta3VsJ6Y1OqVyHtvuAL7K_gKOVmsYV47F_hqaNPlHKOdi6Y_C8Xjw==
      - INFLUXDB_ORG=US_Stock_Data
      - INFLUXDB_BUCKET=fifteenmin_stockdata
    ports:
      - "8002:8002"
    depends_on:
      - kafka
    networks:
      - stock_pipeline

  file_writer:
    build:
      context: ./file_writer_service
      dockerfile: Dockerfile
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPICS_PROCESSED_DAILY=processed-daily
      - KAFKA_TOPICS_PROCESSED_15MIN=processed-15min
      - KAFKA_TOPICS_PROCESSED_OPTIONS=processed-options
    ports:
      - "8003:8003"
    depends_on:
      - kafka
      - minio
    networks:
      - stock_pipeline

  monitoring:
    build:
      context: ./monitoring_service
      dockerfile: Dockerfile
    ports:
      - "8004:8004"
    networks:
      - stock_pipeline

  observability:
    build:
      context: ./observability_service
      dockerfile: Dockerfile
    ports:
      - "8005:8005"
    depends_on:
      - prometheus
      - grafana
    networks:
      - stock_pipeline

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092

    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions.sh", "--bootstrap-server", "kafka:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - stock_pipeline

  kafka-init:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - kafka
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      for i in {1..30}; do
        kafka-broker-api-versions --bootstrap-server kafka:9092 && break
        echo 'Waiting for Kafka to be ready...'
        sleep 5
      done

      kafka-topics --create --topic daily-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
      kafka-topics --create --topic 15min-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
      kafka-topics --create --topic options-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
      kafka-topics --create --topic historical-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1

      kafka-topics --create --topic processed-daily-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
      kafka-topics --create --topic processed-15min-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
      kafka-topics --create --topic processed-options-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
      kafka-topics --create --topic processed-historical-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1

      kafka-topics --create --topic processed-file-daily-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
      kafka-topics --create --topic processed-file-15min-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
      kafka-topics --create --topic processed-file-options-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
      kafka-topics --create --topic processed-file-historical-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1

      echo 'Kafka topics created!'
      "
    networks:
      - stock_pipeline

      
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - stock_pipeline

  minio:
    image: minio/minio:latest
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    networks:
      - stock_pipeline

  prometheus:
    image: prom/prometheus:v2.54.1
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - stock_pipeline
      - monitoring


  grafana:
    image: grafana/grafana:11.2.2
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - stock_pipeline
      - monitoring
  
  node_exporter:
    image: prom/node-exporter:latest
    container_name: node_exporter
    ports:
      - "9100:9100"
    networks:
      - monitoring

volumes:
  mysql_data:
  influxdb_data:
  minio_data:
  grafana_data:

networks:
  stock_pipeline:
    driver: bridge
  monitoring:
    driver: bridge